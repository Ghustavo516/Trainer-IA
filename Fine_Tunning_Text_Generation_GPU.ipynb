{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Treinando o modelo"
      ],
      "metadata": {
        "id": "rAexlhBsPlIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixe as dependências\n",
        "!pip install torch transformers datasets\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "K3ODRnwWnjLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2yGN3TgAsAXw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "FUuWiooJgKPQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar se a GPU está disponível\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Treinando na: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHhixmojdDYz",
        "outputId": "d7279153-411c-46ee-d6bb-29aa5dcc8dfc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando na: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o modelo e tokenizer do Hugging Face\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w7PAO04ydFG9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mover o modelo para a GPU\n",
        "model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "206_YuaTdHWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar seu dataset (substitua com seu caminho correto)\n",
        "dataset = load_dataset('json', data_files='./text_generator_dataset.json')"
      ],
      "metadata": {
        "id": "4nkKyBH3dI2w"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função de tokenização\n",
        "def tokenize_function(examples):\n",
        "    # Tokeniza as perguntas (input) e as respostas (labels)\n",
        "    question_encodings = tokenizer(examples['question'], padding='max_length', truncation=True, max_length=512)\n",
        "    answer_encodings = tokenizer(examples['answer'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "    # Adiciona as labels\n",
        "    question_encodings['labels'] = answer_encodings['input_ids']\n",
        "\n",
        "    return question_encodings\n"
      ],
      "metadata": {
        "id": "mkeQOY1SdP5I"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar a tokenização\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "oqcua7QXdRgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir o dataset em treino e validação\n",
        "train_dataset, eval_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.1).values()"
      ],
      "metadata": {
        "id": "cwlQU_HJeMPf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",              # Diretório de saída\n",
        "    num_train_epochs=3,                  # Número de épocas\n",
        "    per_device_train_batch_size=8,       # Tamanho do lote\n",
        "    per_device_eval_batch_size=8,        # Tamanho do lote para validação\n",
        "    warmup_steps=500,                    # Passos de aquecimento\n",
        "    weight_decay=0.01,                   # Decaimento de peso\n",
        "    logging_dir=\"./logs\",                # Diretório para logs\n",
        "    fp16=True# Passos para log\n",
        ")"
      ],
      "metadata": {
        "id": "KjX2I6VEeaNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # Seu modelo treinado\n",
        "    args=training_args,                  # Argumentos de treinamento\n",
        "    train_dataset=train_dataset,         # Dataset de treinamento\n",
        "    eval_dataset=eval_dataset            # Dataset de validação\n",
        ")"
      ],
      "metadata": {
        "id": "hCT0UMqef9Wf"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar o treinamento\n",
        "trainer.train()\n",
        "\n",
        "# Salvando o modelo no diretório especificado\n",
        "trainer.save_model()\n",
        "\n",
        "# Caso queira salvar o tokenizador junto com o modelo:\n",
        "model.save_pretrained(training_args.output_dir, safe_serialization=True)\n",
        "tokenizer.save_pretrained(training_args.output_dir)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HMniJQDMgAH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando o modelo"
      ],
      "metadata": {
        "id": "V0vkIDV4PdHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Carregar o modelo e o tokenizador\n",
        "model_name = \"./results\"  # Caminho do seu modelo treinado\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Função para gerar a resposta\n",
        "def generate_response(question, answer):\n",
        "    # Criando o prompt para gerar a frase no formato desejado\n",
        "    prompt = f\"Pergunta: {question}\\nResposta: {answer}\\nFrase formal: O indicador ROE da empresa ROMI3 é {answer}\"\n",
        "\n",
        "    # Tokenizar o prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Gerar a resposta\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_new_tokens=50,  # Definir o número máximo de novos tokens que o modelo pode gerar\n",
        "            num_beams=5,  # Beam search para melhorar a qualidade\n",
        "            no_repeat_ngram_size=2,  # Evitar repetições\n",
        "            top_k=50,  # Amostragem\n",
        "            top_p=0.95,  # Probabilidade acumulada\n",
        "            temperature=0.7,  # Controle de criatividade\n",
        "            do_sample=True,  # Ativa amostragem\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decodificar a resposta gerada\n",
        "    output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Exemplo de pergunta e resposta\n",
        "question = \"Qual o valor do indicador ROE da empresa ROMI3?\"\n",
        "answer = \"10.25\"\n",
        "\n",
        "# Gerar a resposta\n",
        "generated_answer = generate_response(question, answer)\n",
        "\n",
        "# Mostrar a resposta gerada\n",
        "print(f\"Resposta gerada: {generated_answer}\")\n"
      ],
      "metadata": {
        "id": "frakD99pF-Xf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}