{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1UVX_t0FK6x-7bnUUwJmUcOkpsQC2SYPI",
      "authorship_tag": "ABX9TyNPnZaH1WPpnoYy6f+1lapg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch datasets transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NLDculoq9_qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "-_i6K-y7FHMZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "SZP8Gn-BFkWP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o tokenizer e o modelo\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"unicamp-dl/ptt5-small-portuguese-vocab\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"unicamp-dl/ptt5-small-portuguese-vocab\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RRB_YOUqFJpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar se a GPU está disponível e mover o modelo para a GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "oc-4tEfZJQOX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o dataset a partir de um arquivo JSON\n",
        "with open('text_to_text.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "Yj7xwdcHFMMB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preparar os dados\n",
        "def prepare_data(data):\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "\n",
        "    for item in data:\n",
        "        question = item['question']\n",
        "        answer = item['answer']\n",
        "\n",
        "        # Formatar a entrada e saída no formato desejado\n",
        "        input_texts.append(f\"question: {question}\")\n",
        "        target_texts.append(f\"answer: {answer}\")\n",
        "\n",
        "    return input_texts, target_texts"
      ],
      "metadata": {
        "id": "BMkHEkBUFQFz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts, target_texts = prepare_data(data)"
      ],
      "metadata": {
        "id": "tmUrwLACFS1w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar o dataset em treino e validação (80% treino, 20% validação)\n",
        "train_size = int(0.8 * len(input_texts))\n",
        "train_data = input_texts[:train_size], target_texts[:train_size]\n",
        "val_data = input_texts[train_size:], target_texts[train_size:]"
      ],
      "metadata": {
        "id": "e6RH7K03FWj-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter para o formato Dataset do Hugging Face\n",
        "train_dataset = Dataset.from_dict({\"input_texts\": train_data[0], \"target_texts\": train_data[1]})\n",
        "val_dataset = Dataset.from_dict({\"input_texts\": val_data[0], \"target_texts\": val_data[1]})"
      ],
      "metadata": {
        "id": "lZXlqhvjFYRT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizar os dados\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples['input_texts'], max_length=512, truncation=True, padding='max_length')\n",
        "    labels = tokenizer(examples['target_texts'], max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "Cag8lKJCFabl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "xn5x8aG-FcUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir os parâmetros de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='text',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    fp16=True,  # Habilitar precisão mista\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        ")"
      ],
      "metadata": {
        "id": "7nGOHvMCFfHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usar o Trainer do Hugging Face para treinar o modelo\n",
        "trainer = Trainer(\n",
        "    model=model,                     # Modelo a ser treinado\n",
        "    args=training_args,              # Argumentos de treinamento\n",
        "    train_dataset=train_dataset,     # Dataset de treino\n",
        "    eval_dataset=val_dataset         # Dataset de validação\n",
        ")"
      ],
      "metadata": {
        "id": "IKuyg33DFg2a"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciar o treinamento\n",
        "trainer.train()\n",
        "\n",
        "# Salvando o modelo no diretório especificado\n",
        "trainer.save_model()\n",
        "\n",
        "# Salvar o modelo treinado\n",
        "model.save_pretrained(training_args.output_dir, safe_serialization=True)\n",
        "tokenizer.save_pretrained(training_args.output_dir)"
      ],
      "metadata": {
        "id": "_N0ypJKuFiHr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando o modelo"
      ],
      "metadata": {
        "id": "S1YavqY008Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "# Carregando o tokenizer e o modelo\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/ptt5-large-portuguese-vocab\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"unicamp-dl/ptt5-large-portuguese-vocab\")\n",
        "\n",
        "# Texto de entrada\n",
        "input_text = \"A ciência e a tecnologia têm avançado rapidamente no Brasil. Recentemente,\"\n",
        "\n",
        "# Tokenizando o texto de entrada\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Gerando o texto com o modelo. Vou definir um tamanho menor para o max_length\n",
        "# e garantir que apenas uma sequência seja retornada com num_return_sequences=1.\n",
        "# Ajuste os parâmetros conforme necessário para a tarefa que você está realizando.\n",
        "outputs = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "\n",
        "# Decodificando a saída para obter o texto gerado\n",
        "generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "# Imprimindo o texto gerado\n",
        "for text in generated_texts:\n",
        "    print(text)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wje3aJ8Szn1m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}